---
title: "Ames Housing Project - Exploratory Analysis"
author: "Quyen Duong, Amin Jamali, Camille Nigon, Pascal Waser"
date: "Last edited `r format(Sys.time(), '%d %B %Y')`"
output: 
  html_document:
    toc: true
    toc_float: 
      collapsed: false 
      smooth_scroll: false
    toc_depth: 4
    number_section: true 
---
<style type="text/css">

body{ /* Normal  */
      font-size: 12px;
}
code.r{
  font-size: 11px;
}
pre {
  font-size: 11px
}
td {  /* Table  */
  font-size: 12px;
}
h1.title {
  font-size: 32px;
  font-weight: bold;
  text-align: center;
  color: #2A5A7A;
  opacity: 0.8;
}
h4.author { 
  font-size: 15px;
  color: #468faf;
  text-align: center;
}
h4.date { 
  font-size: 16px;
  font-family: "Times New Roman", Times, serif;
  color: DarkRed;
  text-align: center;
}
h1 { /* Header 1 */
  font-size: 22px;
  color: Black;
  font-weight: bold;
}
h2 { /* Header 2 */
    font-size: 18px;
  color: Black;
  font-weight: bold
}
h3 { /* Header 3 */
  font-size: 15px;
  color: Black;
  font-weight: bold
}

</style>

```{r setup, include=FALSE}
# Set the default mode for all the chunks
knitr::opts_chunk$set(message = FALSE, warning = FALSE, fig.align = "center", 
                      fig.height = 5, fig.width = 8)
```

```{r load-packages, include = FALSE}
# Load packages
library(dplyr)
library(ggplot2)
library(tidyverse)
library(scales)                      # formatting numbers on axis ggplot
library(gridExtra)
library(tidyr)
library(readr)
library(knitr)
library(lubridate)                   # to format date
library(egg)                         # plot layout 
library(PerformanceAnalytics)        # correlation matrix
library(GGally)                      # correlation matrix
library(corrplot)                    # correlation matrix
library(Hmisc)                       # correlation matrix
library(psych)                       # summary statistics
library(caret)                       # split data train-test
library(broom)                       # visualize fitted linear with categorical variable
library(MASS)
library(conflicted)                  # set conflict
library(ggthemes)
library(grid)
library(leaps)                       # stepwise selection
library(caret)                       # splitting train-test, SVM
library(forecast)                    # calculate RMSE

# Set conflict preference
conflict_prefer("filter", "dplyr")
conflict_prefer("select", "dplyr")

# Deactivate scientific notation
# options(scipen = 999)
```

```{r set-up-theme-for-ggplot, include = FALSE}
my_theme_general <- theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
        text = element_text(size = 10, color = "#495057"),
        panel.grid.major = element_line(linetype = "dashed"),
        panel.grid.minor = element_blank()
    ) 

my_theme_bar_chart <- theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
        plot.subtitle = element_text(hjust = 0.5),
        text = element_text(size = 10, color = "#495057"),
        panel.grid.minor = element_blank(),
        panel.grid.major.x = element_line(linetype = "dashed"),
        panel.grid.major.y = element_blank()
    )

my_theme_heat_map <-  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold"),
    text = element_text(size = 10, color = "#495057"),
    axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)
    )

my_blank_theme <- theme_minimal()+
  theme(
  axis.title.x = element_blank(),
  axis.title.y = element_blank(),
  panel.border = element_blank(),
  panel.grid=element_blank(),
  axis.ticks = element_blank(),
  plot.title=element_text(size=14, face="bold")
  )
```

# Motivation and purpose

The study attempts to predict the house price in Ames, Iowa. To predict the house price, several machine learning methods will be used such as linear model (LM), Generalized Linear Model (GLM), Generalized Additive Model (GAM), Neural Network (NN), Support Vector Machine (SVM) and Optimization. Yet, before predicting price and applying models, it's vital to explore and understand the data. 

# Data comprehension

```{r load-data, include = FALSE}
# Load data
d_house <- read.csv("dataset/AmesHousing.csv", stringsAsFactors = TRUE) %>% 
  as_tibble()

# Check the variables
glimpse(d_house)

# Write a new Excel with all variable name and check if they are important to the house price
# names(d_house) %>% 
#   as_tibble() %>% 
#   writexl::write_xlsx("variables_intuition_analysis.xlsx")

# Exclude the irrelevant variables
d_house <- d_house %>% 
  select(-c(Order, PID))
```

There are 82 variables and 2930 observations in the data set. As there are many variables, we first have to examine and make a intuition analysis on every variable and its relevance to the main variable ´SalePrice´. Although this process can be very time-consuming, it shed the light to grasp the data better. How did we do that? Below are the step explanation:

* Reading and understanding every variable description
* Make a new Excel with two columns: variable name and importance to sale price (Yes/No). The decision of Yes/No is grounded on intuition, expectation and some house-price related articles. To be more specific, we question these following questions: which factor matters when we buy a house? How important would this factor be? Does this factor appear in other variables?

The intuition-based Excel file can be found in the `variables_intuition_analysis.xlsx`. At the first glimpse, we marked these might-be-relevant features to the SalePrice: MS.Zoning, Lot.Area, Street, Lot.Shape, Neighborhood, Bldg.Type, House.Style, Overall.Qual, Year.Built, Foundation, Total.Bsmt.SF, TotRms.AbvGrd, Mo.Sold, Yr.Sold, Garage.Cars, Sale.Condition.

After thinking of potential features, let's inspect the response variable in the next part. 

# The response variable - Sale price 

The scope of this analysis is to predict the housing sales price based on appropriate features. First, we plot the histogram and box plot of the sale price (i.e. the dependent variable).

```{r histogram-sales-price}
# Histogram
histogram_sale_price <- ggplot(d_house, mapping = aes(x = SalePrice)) +
  geom_histogram(aes(y=..density..), color="black", fill = "grey") +
  geom_density(alpha=.2, color ="blue") +
  scale_x_continuous(labels = comma) +
  labs(title = "Histogram of sale price") +
  my_theme_general

# Boxplot
boxplot_sale_price <- ggplot(d_house, mapping = aes(x = "", y = SalePrice)) +
  geom_boxplot(color = "black", fill = "grey") +
  scale_y_continuous(labels = comma) +
  coord_flip() +
   xlab("") +
  my_theme_general

# Display two plots together
egg::ggarrange(histogram_sale_price, boxplot_sale_price, heights = 2:1)

# Descriptive statistics summary of SalePrice
stats_SalePrice <- psych::describe(d_house$SalePrice)
```

The distribution of sale prices displays right-skewed with a long tail. The skew of `r stats_SalePrice$skew` confirms that the data are highly confirmed. Besides, the Kurtosis is `r stats_SalePrice$kurtosis`, i.e. Leptokurtic with a longer distribution and fatter tails. Some houses are very expensive. We have to carefully consider whether these expensive houses are outliers. Instead of dropping outliers, we apply the log transformation for sale prices. 

From the beginning to this point, we just follow the intuition for variable selection. However, this method can be too subjective. Hence, a more objective and systematic way need to be executed as the section below. Nevertheless, we still consider our initial thoughts as a double-check criteria and critical thinking in supplement with the multivariate study.    

# Numerical variable analysis

## Correlation matrix 

```{r corr-matrix}
# Overview correlation matrix of all numerical variables
d_house_numeric <- d_house %>% 
  select(where(is.numeric))

# Missing value check in numerical features -> not so many NA
round(colMeans(is.na(d_house_numeric)), 2) %>%  
  as.data.frame()

# Overview histograms of all numerical variables
# hist.data.frame(d_house_numeric)

# Create correlation matrix, use = complete to obmit NA, because not so many NA
m_cor <- cor(d_house_numeric, use = "complete") 

# Correlogram
corrplot(m_cor, method = "color", addCoef.col="black",
         order = "alphabet", number.cex=0.35, tl.cex = 0.55)
```

The correlation matrix shows some dark blue areas where variables are highly correlated to each other. To be more specific, `Garage.Area` is highly correlated `Garage.Cars` or other pairs such  `Gr.Liv.Area` with `TotRms.AbvGrd`; `Garage.Yr.Blt` with `Year.Built`. These pairs indicate the strong correlation and multi-collinearity. When we check these variables' meaning, they give almost similar information. Here, the correlation matrix is a crucial method to detect the multi-collinear problems in the feature selection process.

```{r corr-SalePrice}
# Calculate correlation matrix for all numerical features against SalePrice
m_cor_SalePrice <- round(
  cor(d_house_numeric, d_house["SalePrice"], use = "complete", ) ,2)

# Transform into tibble and sort correlation in a descending order
d_cor_SalePrice <- tibble(var_name = row.names(m_cor_SalePrice), 
       SalePrice = m_cor_SalePrice[, "SalePrice"]) %>% 
  arrange(desc(SalePrice))
d_cor_SalePrice 
```

Both variables have long tails to the right and many outliers. 

Remarkably, `Lot.Area`, the lot size in square feet is one of the variable we considered as important at the first sight but its correlation to the `SalePrice` is only 0.31. Let's look at the scatter plot of these two variables:

```{r scatter-SalePrice-Lot.Area}
# Scatter plot SalePrice & Lot.Area
scatter_price_lot <- ggplot(d_house, aes(x = Lot.Area, y = SalePrice)) + 
  geom_point() 

# Scatter plot SalePrice & Lot.Area but change the x & y range
scatter_price_lot2 <- ggplot(d_house, aes(x = Lot.Area, y = SalePrice)) + 
  geom_point() +
  xlim(0, 50000) +
  ylim(0, 500000)

# Plot the two side-by-side
egg::ggarrange(scatter_price_lot, scatter_price_lot2, ncol=2, nrow = 1)
```

The two plots show the same information but the right scatter displays the removal of outliers. The `Lot.Area` data points stay mostly at the smaller value and their price varies vertically for the same lot space.

## Top 10 variables correlated to SalePrice

The top 10 numerical features with the absolute highest correlation with the `SalePrice` are now selected and we produce a SalePrice correlation matrix as below:  

```{r top10-numerical-correlation}
# Make df of top 10 numerical variables with the absolute highest correlation to SalePrice
d_cor_top_10 <- d_cor_SalePrice %>%  
  mutate(abs_corr = abs(SalePrice)) %>% 
  arrange(desc(abs_corr)) %>% 
  slice(1:10)

# Make vector with variable name
v_top_10_cor <- d_cor_top_10 %>% 
  # Select the variable names and convert to vector 
  pull(var_name)

# Correlation matrix of top 10
m_cor_top_10 <- cor(d_house_numeric %>% 
               select(all_of(v_top_10_cor)), 
             use = "complete") 

# Correlogram
corrplot(m_cor_top_10, method = "color", addCoef.col="black", type = 'lower',
         order = "alphabet", number.cex=0.55, tl.cex = 0.55)

# Quick look on the scatter plot in pair for the top 10
ggpairs(d_house_numeric %>% select(all_of(v_top_10_cor)))
```

From the new correlation matrix, some important findings are found:

* `Garage.Area` and `Garage.Cars` are correlated highly with the dependent variable but they have the same information. Here, we keep the `Garage.Cars` as its higher correlation.  
* `SalePrice` is strongly correlated to `Gr.Liv.Area`, `Overall.Qual`, `Total.Bsmt.SF` 
* `Full.Bath` means full bathrooms above grade/ground. It seems to be strange here
* `BsmtFin.SF.1` and `Total.Bsmt.SF` display the related information, we choose `Total.Bsmt.SF` in this case
* `TotRms.AbvGrd` and `Gr.Liv.Area` are similar. Here, we choose `Gr.Liv.Area`
* `Year.Built`: 0.55 - moderately correlated to `SalePrice` 

After choosing the variables, let's make scatter plots to check the relationship between SalePrice and other variables

```{r scatter-plots-selected-var-vs-SalePrice}
# Selected numerical variables
d_select_num <- d_house_numeric %>% 
  select(SalePrice, Garage.Cars, Gr.Liv.Area, Overall.Qual, Total.Bsmt.SF, Full.Bath, Year.Built)

# list of selected variables 
l_vars <- c("Garage.Cars",  "Gr.Liv.Area", "Overall.Qual", "Total.Bsmt.SF", "Full.Bath",  "Year.Built")

# empty list for scatter plot
plotlist <- list()

# For loop to get several scatter plots at once with method = lm
for (idx in 1:length(l_vars)) {
  this_var <- l_vars[idx]
  p <- ggplot(d_select_num, aes_string(x = this_var, y = "SalePrice")) +
    geom_point() +
    geom_smooth(method = lm)
  plotlist[[idx]] <- p
} 

# 6 scatter plots at once
egg::ggarrange(plotlist[[1]], plotlist[[2]], 
               plotlist[[3]], plotlist[[4]],
               plotlist[[5]], plotlist[[6]], ncol=3, nrow = 2)
```

In general, some plots indicate that all of these variables have a linear positive relationship with the dependent variable `SalePrice`. 

```{r scatter-plots-version-2}
# empty list for scatter plot
plotlist2 <- list()

# For loop to get several scatter plots at once with not linear line
for (idx in 1:length(l_vars)) {
  this_var <- l_vars[idx]
  p <- ggplot(d_select_num, aes_string(x = this_var, y = "SalePrice")) +
    geom_point() +
    geom_smooth()
  plotlist2[[idx]] <- p
} 

# 6 scatter plots at once
egg::ggarrange(plotlist2[[1]], plotlist2[[2]], 
               plotlist2[[3]], plotlist2[[4]],
               plotlist2[[5]], plotlist2[[6]], ncol=3, nrow = 2)
```

# Missing value

It's important to inspect the missing values in each variable. How is the ratio of missing value in each feature? What does the missing value mean in each case? Are missing values random? These questions are vital when we deal with the missing value. Removing them immediately without reflecting can cause data size reduction and bias.

```{r missing-values}
# Number of NA values in each column
d_number_missing_values <- colSums(is.na(d_house)) %>%  
  as.data.frame() 
colnames(d_number_missing_values) <- c("number")

# Percentage of NA values in each column
d_perc_missing_values <- round(colMeans(is.na(d_house)), 2) %>%  
  as.data.frame()
colnames(d_perc_missing_values) <- c("ratio")
d_perc_missing_values

# A summary table of missing values in each column
d_summary_missing_value <- cbind(d_number_missing_values, d_perc_missing_values)

# Change the summary missing values to tibble
d_summary_missing_value <- tibble(var_name = row.names(d_summary_missing_value), 
       n_missing = d_summary_missing_value[, "number"],
       perc_missing = d_summary_missing_value[, "ratio"]) %>% 
  arrange(desc(perc_missing)) 
d_summary_missing_value 

# Exclude the columns with 49% missing value
d_house <- d_house %>% 
  select(-c(Alley, Fireplace.Qu, Pool.QC, Fence, Misc.Feature))

# Remove the 1 NA rows from two important columns
d_house <- d_house %>% drop_na(Garage.Cars, Total.Bsmt.SF)
```

After examine the missing ratio, here are some decisions:

* Excluding all the columns with more than 49% of missing values, namely `Pool.QC`, `Misc.Feature`, `Alley`, `Fence` and `Fireplace.Qu`.Filling NA value is not applied because these variables are not so important and not the main factors to think about when we buy a house.
* Group of variables with "Garage": 5% are missing in these columns. Since we chose only `Garage.Cars` for the model in the previous part, we do not care about the rest.
* Group of variables with "Bsmt": similar principle as the group of "Garage"

# Categorical variables

In this section, we check the effects of different categorical features that are potentially relevant for the model. Before applying function drop1(), we visualize SalePrice with several categorical variables

## Visualizations of categorical variables

```{r Boxplot-Fuction}
# Make a function to draw all box plots
fill <- "#FFFFFF"
line <- "#000000"

boxplot_theme <- function(variable_input, title, variable) {
  output <- ggplot(d_house, aes(x = variable_input, y = SalePrice)) +
    geom_boxplot(fill = fill, colour = line) +
    ggtitle(title) +
    theme_minimal() +
    scale_y_continuous(labels = comma) +
    labs(x = paste0("\n", variable), y = "Price\n") + 
    theme(axis.text.x = element_text(angle = 45, vjust = 0.5, hjust=1),
          legend.position = "bottom", legend.direction = "horizontal",
          legend.box = "horizontal",
          legend.key.size = unit(1, "cm"),
          plot.title = element_text(family="Tahoma", hjust = 0.5),
          text = element_text(family = "Tahoma"),
          axis.title = element_text(size = 10),
          legend.text = element_text(size = 9),
          legend.title=element_text(face = "bold"))

  return(output)
}  
```

### MS.Zoning

`SalePrice` seems to vary among different `MSZoning`. Here, `MSZoning` identifies the general zoning classification of the sales such as agriculture, commercial, industrial. Especially Residential with low density (RL) and residential with medium density (RM) are strongly right skewed.

```{r ms.zoning}
# Boxplot price in ms zonings
title <- "General Zoning Classification"
variable <- "MS Zoning"
boxplot_theme(d_house$MS.Zoning, title, variable)
```

### Lot Shape

The general shapes of the property appear to be similar. With the exception that properties with a regular shape has a lower value compared to an irregular shape.

```{r LotShape}
# Boxplot price in different lot shapes
title <- "General shape of property"
variable <- "Lot Shape"
boxplot_theme(d_house$Lot.Shape, title, variable)
```

### Neighborhood

We identified especially in the neighborhoods strong variances regarding the prices. Especially Northridge (NoRidge) has a high price per property and shows some outliers in the direction of an higher price. 

```{r Neighborhood}
# Boxplot price in different neighborhoods
title <- "Physical locations within Ames city limits"
variable <- "Neighborhood"
boxplot_theme(d_house$Neighborhood, title, variable)
```

### Bldg.Type

The type of dwelling or house shows no radical differences for each category's median price. However, Single-family Detached (1Fam) has the highest potential and many tails to the higher prices.

```{r Bldg.Type}
# Boxplot price in different building types
title <- "Type of building"
variable <- "Building Type"
boxplot_theme(d_house$Bldg.Type, title, variable)
```

### House.Style

The same for House Style. The sales price for one story or two story is not strong impacted by the house style. 

```{r House.Style}
# Boxplot price in different house styles
title <- "Style of dwelling"
variable <- "HouseStyle"
boxplot_theme(d_house$House.Style, title, variable)
```

### Foundation

The `Foundation` variable consists of Brick & Tile, Cinder Block, Poured Contrete, Slab, Stone, Wood. The price seems to differ in different type of foundation but again, this can be a factor that was influenced by the `Year.Built`.

```{r Foundation}
# Boxplot price in different house styles
title <- "Foundation"
variable <- "Foundation"
boxplot_theme(d_house$Foundation, title, variable)
```

### Functional

The `Functional` variable consists of Brick & Tile, Cinder Block, Poured Contrete, Slab, Stone, Wood. The price seems to differ in different type of foundation but again, this can be a factor that was influenced by the `Year.Built`.

```{r Functional}
# Boxplot price in different house styles
title <- "Functional"
variable <- "Functional"
boxplot_theme(d_house$Functional, title, variable)
```

### Sale conditions

This variable indicate the condition of sale such as normal sale, abnormal sale (trade, short sale), sale between family members, adjoing land purchases etc. The median of the sales price is the highest among the Sales condition which was not completed when last assessed (associated with new homes). 

```{r Sale conditions}
title <- "Condition of sale"
variable <- "Sale Condition"
boxplot_theme(d_house$Sale.Condition, title, variable)
```

## Testing the effect of categorical variables

After several visualizations and descriptive analysis, let's fit the model with all highly potential categorical variables and then apply drop1() function to test their significance.

```{r drop1-categorical}
# Select all categorical might be relevant
d_house_selected_categorical <- d_house %>% 
  select(MS.Zoning, Neighborhood, Street, House.Style, Sale.Condition)

# Fit model with only these categorical variables
lm_cate_1 <- lm(log(SalePrice) ~ MS.Zoning + Neighborhood + Street + Foundation + 
                Bldg.Type + House.Style + Sale.Condition + Functional, 
                data = d_house)

# Check their significance effects
drop1(lm_cate_1, test = "F")
```

From the p-value obtained, `Street` does not seem to have a relevant effect on the response variable.

## Predictors of focus

As we have seen so far, the linear model describes relationship between the response variable $Y$ and the predictors $X_1,X_2,...X_n$. However, not all explanatory variables play an important role in predicting the random variable. Now, the question is if omitting a predictor would affect the degree to which the model fits the data, substantially deteriorated or not? As we will not fit the model with 74 predictors, it's vital to choose some focus variables, which are more potential and relevant for our model. 

### Stepwise selection

Command `regsubsets` from the library `leaps` runs the whole procedure automatically. Wherever TRUE is written, the corresponding explanatory variable is contained in model.

```{r step-wise-selection}
# Choose all variables exclude some variables starting with "Bsmt" because cannot run stepwise selection
d_exclude <- d_house %>% 
  select(!starts_with("Bsmt"))

# Fit all variables
lm_full <- lm(log(SalePrice) ~ ., data = d_exclude)

# Forward selection
reg_model <- regsubsets(log(SalePrice) ~ ., data = d_exclude, 
                        na.action=na.exclude, method = "forward", nvmax = 11)
reg_sum <- summary(reg_model)

reg_sum$which
```

Based on step wise selection, these are the features containing in the model: Gr.Liv.Area, Overall.Qual, Overall.Cond, Year.Built, Total.Bsmt.SF, Fireplaces, Garage.Cars. Some categorical variables with TRUE in one of the sub-categories are MS.Zoning, Neighborhood, Bldg.Type, Functional

### Testing the effects all potential features together

The function `drop1()` gives an overview about the comparison of original model to the model where one explanatory variable was omitted.  

```{r lm-fit-all-potential}
# Fitting all potential features together
lm_fit_focus <- lm(log(SalePrice) ~ Gr.Liv.Area + MS.Zoning + Neighborhood + 
                      Overall.Qual + Overall.Cond + Bldg.Type + Year.Built + 
                      Foundation + Total.Bsmt.SF + Fireplaces + Functional + 
                      House.Style + Garage.Cars + Sale.Condition, 
                    data = d_house)

# Testing all potential features together
drop1(lm_fit_focus, test = "F")

# Summary of the model 
summary(lm_fit_focus)
```

### Predictor summary

To sum up, we started with intuition analysis, numerical variable, then categorical variable analysis. Afterward, the step wise selection were conducted. Next, the `drop1` function to test the relevant effect of different variables on the response variables. Below is the finding summary of variables, which we focus from different methods:

![](summary_predictor_of_focus.png)

# Train and test data

The data are divided into 70% train data and 30% test data because there are 2930 observations, which are large enough for that ratio. Moreover, we use function `createDataPartition()`, which helps to have more random split of the data.  

From the scatter plots in the numerical analysis part, there are some outliers, which do not follow the trend in the Gr.Liv.Area and Total.Bsmt.SF. Thus, we remove them first before fitting the model because keeping them will make the errors become extremely high and there are only 3 outliers. 

```{r train-test}
set.seed(12)
# We choose 70% for train data
indices <- createDataPartition(d_house$SalePrice, p = 0.70, list = F)

# Remove outliers from Gr.Liv.Area & Total.Bmst.SF & Garage.Cars
d_house <- d_house %>%
  filter(Gr.Liv.Area < 4500 & Total.Bsmt.SF < 4500)

# Train data
d_train <- d_house %>%
  slice(indices)

# Test data
d_test <- d_house %>%
  slice(-indices) 
```

# Linear model

## Fitting several models

To choose the best linear model, we fit several models on the train data and check the errors on the test data. At the end, we chose the best result for linear model. The fitting model process, we will use out sample cross validation, meaning that fitting model with train data and predict on the test data:

```{r lm-model-1}
# Model based on stepwise variables
lm1_train <- lm(log(SalePrice) ~ Gr.Liv.Area + Overall.Qual + Overall.Cond + 
                  Year.Built + Total.Bsmt.SF + Fireplaces + Garage.Cars + 
                  MS.Zoning + Bldg.Type + Functional, 
                data = d_train)
# Predict the model 1 on test data
pred_lm1_test <- predict(lm1_train, newdata = d_test)

# R^2
r_squared_lm1 <- cor(exp(pred_lm1_test), d_test$SalePrice)^2
# RMSE
## Way 1: manually
sqrt(mean((exp(pred_lm1_test) - d_test$SalePrice)^2))
## Way 2: by function
accuracy(exp(pred_lm1_test), d_test$SalePrice)

# more info about the model
broom::glance(lm1_train)
```

```{r lm-model-2}
# More simple model 
lm2_train <- lm(log(SalePrice) ~ Gr.Liv.Area + Year.Built + Overall.Qual + 
                  MS.Zoning, data = d_train)
# Predict the model 2 on test data
pred_lm2_test <- predict(lm2_train, newdata = d_test)
# R^2
r_squared_lm2 <- cor(exp(pred_lm2_test), d_test$SalePrice)^2
# RMSE
accuracy(exp(pred_lm2_test), d_test$SalePrice)
# more info about the model
broom::glance(lm2_train)
```

```{r lm-model-3}
# Model  
lm3_train <- lm(log(SalePrice) ~ Gr.Liv.Area + MS.Zoning +
                      Overall.Qual + Bldg.Type + Year.Built + Foundation + 
                      Total.Bsmt.SF + House.Style + Garage.Cars + Sale.Condition,
                na.action=na.omit,
                data = d_train)
# Predict the model 2 on test data
pred_lm3_test <- predict(lm3_train, newdata = d_test)
# R^2
r_squared_lm3 <- cor(exp(pred_lm3_test), d_test$SalePrice)^2
# RMSE
accuracy(exp(pred_lm3_test), d_test$SalePrice)
# more info about the model
broom::glance(lm3_train)
```

```{r lm-model-4}
# Model 
lm4_train <- lm(log(SalePrice) ~ Gr.Liv.Area +
                      Overall.Qual + Bldg.Type + Year.Built +
                      Garage.Cars + Sale.Condition,
                na.action=na.omit,
                data = d_train)
# Predict the model 2 on test data
pred_lm4_test <- predict(lm4_train, newdata = d_test)
# R^2
r_squared_lm4 <- cor(exp(pred_lm4_test), d_test$SalePrice)^2
# RMSE
accuracy(exp(pred_lm4_test), d_test$SalePrice)
# more info about the model
broom::glance(lm4_train)
```

```{r lm-model-5}
# Model with intuition variables
lm5_train <- lm(log(SalePrice) ~ Overall.Qual + Year.Built + Overall.Cond + 
                  MS.Zoning + Bldg.Type + Sale.Condition, 
                data = d_train)

# Predict the model 2 on test data
pred_lm5_test <- predict(lm5_train, newdata = d_test)
# R^2
r_squared_lm5 <- cor(exp(pred_lm5_test), d_test$SalePrice)^2
# RMSE
accuracy(exp(pred_lm5_test), d_test$SalePrice)
# more info about the model
broom::glance(lm5_train)
```

```{r lm-model-6}
# Model with high correlated variables
lm6_train <- lm(log(SalePrice) ~ Gr.Liv.Area + Overall.Qual + Year.Built + 
                  Full.Bath, 
                data = d_train)

# Predict the model 2 on test data
pred_lm6_test <- predict(lm6_train, newdata = d_test)
# R^2
r_squared_lm6 <- cor(exp(pred_lm6_test), d_test$SalePrice)^2
# RMSE
accuracy(exp(pred_lm6_test), d_test$SalePrice)
# more info about the model
broom::glance(lm5_train)
```

```{r lm-model-7}
# Model with variables appearing in several analysis
lm7_train <- lm(log(SalePrice) ~ Gr.Liv.Area + Overall.Qual + Year.Built + 
                  Total.Bsmt.SF + Garage.Cars + MS.Zoning + Bldg.Type + 
                  Functional, 
                data = d_train)
# Predict the model 2 on test data
pred_lm7_test <- predict(lm7_train, newdata = d_test)
# R^2
r_squared_lm7 <- cor(exp(pred_lm7_test), d_test$SalePrice)^2
# RMSE
accuracy(exp(pred_lm7_test), d_test$SalePrice)
# more info about the model
broom::glance(lm7_train)
```

```{r lm-model-8}
# Model altered
lm8_train <- lm(log(SalePrice) ~ Gr.Liv.Area + Overall.Qual + Year.Built  + 
                  Garage.Cars + MS.Zoning + Bldg.Type + Functional, 
                data = d_train)
# Predict the model 2 on test data
pred_lm8_test <- predict(lm8_train, newdata = d_test)
# R^2
r_squared_lm8 <- cor(exp(pred_lm8_test), d_test$SalePrice)^2
# RMSE
accuracy(exp(pred_lm8_test), d_test$SalePrice)
# more info about the model
broom::glance(lm8_train)
```

```{r lm-model-9}
# Model altered
lm9_train <- lm(log(SalePrice) ~ Lot.Area + Gr.Liv.Area + Overall.Qual + Year.Built +
                  Overall.Cond + Bldg.Type + Fireplaces + Functional +
                  Total.Bsmt.SF + Garage.Area, 
                data = d_train)
# Predict the model 2 on test data
pred_lm9_test <- predict(lm9_train, newdata = d_test)
# R^2
r_squared_lm9 <- cor(exp(pred_lm9_test), d_test$SalePrice)^2
# RMSE
accuracy(exp(pred_lm9_test), d_test$SalePrice)
# more info about the model
broom::glance(lm9_train)
```

## The final linear model

After fitting several models above, the final model is as below. In this part, we first fit the model  

```{r lm-final-model-all-data}
# Fit the final model with the whole dataset
lm_final <- lm(log(SalePrice) ~ Gr.Liv.Area + Overall.Qual + Overall.Cond + 
                  Year.Built + Total.Bsmt.SF + Fireplaces + Garage.Cars + 
                  MS.Zoning + Bldg.Type + Functional, 
                data = d_house)
# Predict the model 1 on test data
summary(lm_final)

# texreg::texreg(lm_final)
# texreg::htmlreg(lm_final)
```

### Estimated coefficients

Let’s look at the estimated regression coefficients.

```{r coeff}
# All coefficients
coef(lm_final)

## Interpreting the slope
exp(lm_final$coefficients["(Intercept)"])

## interpreting overall quality slope
(exp(lm_final$coefficients["Overall.Qual"])-1)*100

## building type "Duplex"
(lm_final$coefficients["Bldg.TypeDuplex"])
```

Interpretation 

* The intercept: When every predictors is zero, the house price is `r exp(lm_final$coefficients["(Intercept)"])` (extrapolation)
* The slope of `Overall.Qual`: For every one-scale increase in the overall quality (assuming all other predictors remain constant), the house price increases by about `r (exp(lm_final$coefficients["Overall.Qual"])-1)*100`%.
* Categorical variable `Bldg.Type`: Here, the  1Fam in the building type is used as the reference level. The `r lm_final$coefficients["Bldg.TypeDuplex"]` of Duplex is the difference of the intercept to 1Fam. 

If we are interested in estimating the magnitude of an effect and its uncertainty, let's look at the confident intervals for the `lm_final` model.

```{r lm-final-P-values–confidence-intervals}
confint(lm_final)
```

This final model were chosen based on the lowest RMSE and the highest adjusted R squared from the previous part. Notably, the adjusted R-squared to compare the goodness-of-fit for regression models that contain differing numbers of independent variables. That's why we use this to evaluate the models. 

### Prediction

It's important to examine visually the prediction errors of the model. We present predicted value in the original unit of the house price by applying the function `exp()`. 

```{r lm-final-prediction-plot}
# Make predictions from the model, exponential back the value
d_house_lm <- d_house %>% 
  mutate(pred_lm = exp(predict(lm_final)))

# Plot the predictions (on x axis) versus the house price
ggplot(d_house_lm, aes(x = pred_lm, y = SalePrice)) + 
  geom_point() + 
  geom_abline() +
  labs(title = "The actual Sale Price and the predicted Sale Price from linear model") +
  my_theme_general

# Calculate residuals
d_house_lm$residuals <- d_house_lm$SalePrice - d_house_lm$pred_lm

# Plot predictions (on x-axis) versus the residuals
ggplot(d_house_lm, aes(x = pred_lm, y = residuals)) + 
  geom_pointrange(aes(ymin = 0, ymax = residuals)) + 
  geom_hline(yintercept = 0, linetype = 3) + 
  ggtitle("Residuals vs. Linear Model prediction") +
  my_theme_general
```

Now, we will also plot the gain curve of the lm_final's predictions against actual SalePrice. For situations where order is more important than exact values, the gain curve helps us check if the model's predictions sort in the same order as the true outcome. When the predictions sort in exactly the same order, the relative Gini coefficient is 1. When the model sorts poorly, the relative Gini coefficient is close to zero, or even negative.

```{r lm-final-Gain-Curve}
# Plot the Gain Curve
WVPlots::GainCurvePlot(d_house_lm, "pred_lm", "SalePrice", "lm_final")
```

A relative gini coefficient of 0.96, $\approx$ 1, shows that the model correctly sorts high house price situations from lower ones.

### Assessing model accuracy

#### In-sample performance

We start with “in-sample” performance, which means the accuracy measured on models fitted to all data available.

```{r lm-final-insample-evaluation}
# R squared of the final model
glance_lm_final <- broom::glance(lm_final)
glance_lm_final
# RMSE of the final model
accuracy(exp(predict(lm_final)), d_house$SalePrice)
```

Observations:

* R-squared = `r glance_lm_final$r.squared`
* Adjusted R-squared = `r glance_lm_final$adj.r.squared`

The model explains 89% of the variance. Does this model is also better at making predictions on new observations that have not been used to train the model? Thus, we move to out-sample performance.

#### Out-sample performance

We now fit a part of the data to train the model and then test its predictive performance on a part of data that was splitted aside.

```{r lm-final-out-sample}
# Fit the model on the train data
lm_final_train <- lm(log(SalePrice) ~ Gr.Liv.Area + Overall.Qual + Overall.Cond + 
                  Year.Built + Total.Bsmt.SF + Fireplaces + Garage.Cars + 
                    Bldg.Type + Functional, 
                data = d_train)
# Predict the final model on test data
pred_lm_final_test <- predict(lm_final_train, newdata = d_test)

# R^2
r_squared_lm_final <- cor(exp(pred_lm_final_test), d_test$SalePrice)^2

# RMSE
accuracy_lm_final <- accuracy(exp(pred_lm_final_test), d_test$SalePrice)

# The standard deviation of the outcome
sd(d_house$SalePrice)
```

Observations:

* R-squared out-sample = `r r_squared_lm_final`: even higher than R squared in-sample
* RMSE = `r accuracy_lm_final[,"RMSE"]`: the standard deviation of the prediction errors is much smaller than the standard deviation of the house price (i.e., `r sd(d_house$SalePrice)` USD)
* MAPE = `r accuracy_lm_final[,"MAPE"]`: on average, the model predicts with approximately 10% errors. 

The cross-validation R-squared (measured on out-of-sample) is slightly higher than in-sample R-squared. The chosen linear model is therefore concluded as a good model because it does not overfit the data.   

#### Cross-validation

Previously, we fix the train and test data, does the result change if the data was split differently? To ensure about the model, we will make 3-way cross-validation as follows:

![](cross_validation_illustration.png)

```{r lm-cross-validation}
library(vtreat)

# Create a cross-validation plan with 3 folds (partitions)
splitPlan <- kWayCrossValidation(nrow(d_train), 3, NULL, NULL)

# Remove some observations with very few cases in 2 variables MS.Zoning 
# and Functional 
d_house_lm <- d_house_lm %>%  
  filter(!MS.Zoning %in% c("A (agr)", "I (all)"), 
         !Functional %in% c("Sal", "Sev"))

# Run the 3-fold cross validation plan from splitPlan
k <- 3 # Number of folds
d_house_lm$pred.cv <- 0 
for(i in 1:k) {
  
  split <- splitPlan[[i]]
  model <- lm(log(SalePrice) ~ Gr.Liv.Area + Overall.Qual + Overall.Cond + 
                  Year.Built + Total.Bsmt.SF + Fireplaces + Garage.Cars +
                MS.Zoning + Bldg.Type + Functional, 
              data = d_house_lm[split$train,])
  d_house_lm$pred.cv[split$app] <- predict(model, 
                                           newdata = d_house_lm[split$app, ])
}

# Predict from a full model
d_house_lm$pred <- predict(lm(log(SalePrice) ~ Gr.Liv.Area + Overall.Qual + 
                                Overall.Cond + Year.Built + Total.Bsmt.SF + 
                                Fireplaces + Garage.Cars +
                MS.Zoning + Bldg.Type + Functional, data = d_house_lm))

# Get the rmse of the full model's predictions
ModelMetrics::rmse(d_house_lm$pred, d_house_lm$SalePrice)

# Get the rmse of the cross-validation predictions
ModelMetrics::rmse(d_house_lm$pred.cv, d_house_lm$SalePrice)
```

# Generalised Linear Model (GLM) with family set to Poisson

## Model 1: Predicting number of cars

The house price, our main response variable is a continuous variable. In this part, we will predict another variable which is integer: `Garage.Cars`: The number of cars that a garage can have. 

We fit the model with these predictors: `Garage.Area`, `SalePrice` and `Land.Contour` (Flatness of the property with 4 levels: 1) Lvl: Near Flat/Level, 2) Bnk: Banked - Quick and significant rise from street grade to building, 3) HLS: Hillside - Significant slope from side to side, 4) Low: Depression)

However, this is just an example to fit a poisson model based on our dataset. We are fully aware that the garage cars are not really count in the sense of Poisson distribution assumption. More particularly, the variances of the observations does not linearly increases with the mean value. For instance, the box plot below shows no big difference between groups of Land.Contour. Similarly, we tried to make box plot of `Garage.Cars` with many different categorical variables to investigate. The dataset only consists the car number from 0 to 5. A house commonly does not have 20 cars. Other potential count variables such as `Fireplaces` and `Full.Bath` have the same issue. Hence, we just fit a Poisson model to illustrate:

```{r boxplot-cars-land-contour}
# Boxplot of Garage.Cars with Land.Contour
ggplot(data = d_house, mapping = aes(y = Garage.Cars, x = Land.Contour)) +
  geom_hline(yintercept = 0) + ## to highlight the lower bound of the data
  geom_boxplot() 
```

```{r glm1-poisson}
# Fit the poisson model to predict number of garage cars
glm1_poisson <- glm(Garage.Cars ~ Garage.Area + SalePrice + Land.Contour, 
                   family = "poisson", ## we specify the distribution!
                   data = d_house)

## summary Poisson model
summary(glm1_poisson)

## Coefficient: Land Coutour 
exp(coef(glm1_poisson)["Land.ContourHLS"]) %>% round(digits = 2)

## Coefficient: Garage.Area
exp(coef(glm1_poisson)["Garage.Area"])

## Coefficient: Sale Price
exp(coef(glm1_poisson)["SalePrice"]*10000)
```

Interpretation of Coefficient of Land Coutour:

R takes `Land.ContourBnk` (i.e., Banked - Quick and significant rise from street grade to building) as the reference. Hence, after applying exponential, the hillside (significant slope from side to side) land contour get, on average, 5% more garage cars than the banked land contour. 

For a given home, increasing its number of Garage area by one square feet, would results in about 0.15% more cars. For a given home, increasing its price by 10000 USD, would results in about 3% more cars.

## Model 2: Predicting the house price

```{r boxplot-price-building-type}
# Boxplot of Garage.Cars with Land.Contour
ggplot(data = d_house, mapping = aes(y = SalePrice, x = Bldg.Type)) +
  geom_hline(yintercept = 0) + ## to highlight the lower bound of the data
  geom_boxplot() 
```

```{r glm2-poisson}
# Fit the poisson model to predict house price
glm2_poisson <- glm(SalePrice ~ Gr.Liv.Area + Bldg.Type, 
                   family = "poisson", ## we specify the distribution!
                   data = d_house)

## summary Poisson model 2
summary(glm2_poisson)

## Coefficient: Building type duplex
exp(coef(glm2_poisson)["Bldg.TypeDuplex"]) %>% round(digits = 2)

## Coefficient: Gr.Liv.Area
exp(coef(glm2_poisson)["Gr.Liv.Area"])
```

# Generalised Linear Model (GLM) with family set to Binomial

## Model 1: Predicting groups of price per square feet

In this part, we first calculate the house price per square feet by taking the `SalePrice` dividing by `LotArea`. To apply the binomial of GLM, two groups of price per square feet are created: upper and lower price. Afterward, we compute the ratio of upper price houses in the dataset based on their overall quality. Next, the GLM model is built on the rate of Price.Upper (number of successes) and Price.Lower (number of failures) to fulfill the binomial proportion.      

```{r glm1-create-two-price-groups}
# Create two groups of price according to the overall quality
d_house_glm1 <- d_house %>% 
  mutate(SalePrice.Per.SqrFeet = SalePrice / Lot.Area) %>% 
  group_by(Overall.Qual) %>%
  summarise(
    ## Upper price
    Price.Upper = sum(ifelse(SalePrice.Per.SqrFeet > mean(SalePrice.Per.SqrFeet), 1, 0)),
    ## Lower price
    Price.Lower = sum(ifelse(SalePrice.Per.SqrFeet <= mean(SalePrice.Per.SqrFeet), 1, 0)),
    ## Rate of upper
    Price.Upper.Rate = round(Price.Upper/(Price.Upper + Price.Lower), digits = 2))
```

```{r glm1-binomial-price-per-square-feet}
glm1_binomial <- glm(cbind(Price.Upper, Price.Lower) ~ Overall.Qual,
                    family = "binomial",
                    data = d_house_glm1)

summary(glm1_binomial)
```

After fitting the model, the overall quality seems not play a role in the price-per-square-feet groups. Indeed, its p-value is not significant. Also, the Overall.Qual slope is negative but we could not draw a conclusion since the variable is not significant in this case. However, we will still visualize the results of this model. The red dots are observed value and the predictions are a sequqnece of 100 observations that ranges from one to ten (black dots). 

```{r plot-Effect-Overall-quality}
# Plot Effect-Overall Quality
new_data <- data.frame(Overall.Qual = seq(1, 10, length.out = 100))
new_data$pred_price_upper_ratio <- predict(glm1_binomial, newdata = new_data,
                                 type = "response")

##
ggplot(data = d_house_glm1,
       mapping = aes(y = Price.Upper.Rate,
                     x = Overall.Qual)) + 
  ylim(0,1) +
  geom_hline(yintercept = 0:1, col = "gray") +
  ## predictions for conc 0 --> 10
  geom_point(data = new_data,
               mapping = aes(
      y = pred_price_upper_ratio,
      x = Overall.Qual)) +
  ## actual observations
  geom_point(col = "red", 
             size = 3)
```

As expected, the model does not predict so well. 

## Model 2: Predicting the central air

After the first model, we now attempt to predict whether a central air is in the house. Recall that the center air variable consist of Yes/No. Here, the `Overall.Qual` will be used as the predictor. Before fitting the model, let's check the plot of reposnse and independent variables. 

```{r glm2-central-air-plot}
# Create Center Air: 1 for yes and 0 for no
d_house_glm2 <- d_house %>% 
  mutate(Central.Air.new = ifelse(Central.Air == "Y", 1, 0))

# a logistic regression model and add the fit to the this graph
ggplot(data = d_house_glm2, mapping = aes(y = Central.Air.new, x = Overall.Qual)) +
  geom_point() +
  geom_smooth(method = "glm", 
              se = FALSE,
              method.args = list(family = "binomial")) 
```

There are two only possible outcomes here as yes/no or success/failure (coded as 1 and 0 respectively). The higher overall quality seems to have central air but there are also some low quality observations with central air. Let's fit the model as belows: 

```{r glm2-fitting}
# Fitting model 2 to predict Central.Air
glm2_binomial <- glm(Central.Air.new ~ Overall.Qual,
                data = d_house_glm2,
                family = "binomial")

# Summary of the model
summary(glm2_binomial)
```

The overall quality has a positive effect on the response variable. To interpret the coefficient of overall quality, we apply exponential function. When the overall quality increases by one level, the probability of having central air increases `r round(exp(coef(glm2_binomial)["Overall.Qual"]),1)` times. To evaluate the fitted values with the observed values (binary 0 or 1), it's important to discretize the fitted value into 0 and 1. At this stage, the cutoff of 0.5 is used.  

```{r}
# Discretise the fitted value into 0 and 1 with the cutoff of 0.5
fitted_glm2_binomial <- ifelse(fitted(glm2_binomial) > 0.5,
                               yes = 1, no = 0)

# Make a data frame of observed and fitted values
d_obs_fit_glm2 <- data.frame(obs = d_house_glm2$Central.Air.new,
                             fitted = fitted_glm2_binomial)

# summarise them into a table
table(obs = d_obs_fit_glm2$obs,
      fit = d_obs_fit_glm2$fitted)
```

To compare the observed and fitted values after discretizing, we summarize them in the confusion matrix above. 14 observations were correctly labeled to be No and 2726 observations were correctly labeled to be Yes. Nevertheless, 182 were wrongly classified by the model to be Yes but in reality, they are No (false positives). Besides, the model made mistakes of 3 observations as being No while they were not in reality (false negative). 



